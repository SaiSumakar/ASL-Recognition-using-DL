{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39bb33c1-e2b9-4be1-afb2-40d7b403d84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tkinter import *\n",
    "import cv2\n",
    "from PIL import ImageTk, Image\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b5e28d9-e72c-48bd-89f0-f725aa2ba05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker\n",
    "\n",
    "\n",
    "def Auto_Correct(word):\n",
    "    mySpellChecker = SpellChecker()\n",
    "    return mySpellChecker.correction(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a9899b2-89a0-4839-9e84-bce015606c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "\n",
    "  classifier = None\n",
    "  def __init__(self, Type):\n",
    "    self.classifier = Type\n",
    "    \n",
    "  def build_model(classifier):\n",
    "    \n",
    "\n",
    "    classifier.add(Convolution2D(128, (3, 3), input_shape=(64, 64, 1), activation='relu'))\n",
    "\n",
    "    classifier.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "    classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    classifier.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "    classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    classifier.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "    classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    classifier.add(Dropout(0.5))\n",
    "\n",
    "    classifier.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "    classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    classifier.add(Dropout(0.5))\n",
    "\n",
    "    classifier.add(Flatten())\n",
    "\n",
    "    classifier.add(Dropout(0.5))\n",
    "    \n",
    "    classifier.add(Dense(1024, activation='relu'))\n",
    "    \n",
    "\n",
    "    classifier.add(Dense(29, activation='softmax'))\n",
    "\n",
    "    return classifier\n",
    "\n",
    "  def save_classifier(path, classifier):\n",
    "    classifier.save(path)\n",
    "\n",
    "  def load_classifier(path):\n",
    "    classifier = load_model(path)\n",
    "    return classifier\n",
    "\n",
    "  def predict(classes, classifier, img):\n",
    "    img = cv2.resize(img, (64, 64))\n",
    "    img = img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = img/255.0\n",
    "\n",
    "    pred = classifier.predict(img)\n",
    "    return classes[np.argmax(pred)], pred\n",
    "    \n",
    "\n",
    "class DataGatherer:\n",
    "\n",
    "  def __init__(self, *args):\n",
    "    if len(args) > 0:\n",
    "      self.dir = args[0]\n",
    "    elif len(args) == 0:\n",
    "      self.dir = \"\"\n",
    "\n",
    "  #this function loads the images along with their labels and apply pre-processing function on the images \n",
    "  # and finaly split them into train and test dataset\n",
    "  def load_images(self):\n",
    "    images = []\n",
    "    labels = []\n",
    "    index = -1\n",
    "    folders = sorted(os.listdir(self.dir))\n",
    "    \n",
    "    for folder in folders:\n",
    "      index += 1\n",
    "      \n",
    "      print(\"Loading images from folder \", folder ,\" has started.\")\n",
    "      for image in os.listdir(self.dir + '/' + folder):\n",
    "\n",
    "        img = cv2.imread(self.dir + '/' + folder + '/' + image, 0)\n",
    "        \n",
    "        img = self.edge_detection(img)\n",
    "        img = cv2.resize(img, (64, 64))\n",
    "        img = img_to_array(img)\n",
    "\n",
    "        images.append(img)\n",
    "        labels.append(index)\n",
    "\n",
    "    images = np.array(images)\n",
    "    images = images.astype('float32')/255.0\n",
    "    labels = to_categorical(labels)\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(images, labels, test_size=0.1)\n",
    "\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "  def edge_detection(self, image):\n",
    "    minValue = 70\n",
    "    blur = cv2.GaussianBlur(image,(5,5),2)\n",
    "    th3 = cv2.adaptiveThreshold(blur,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,cv2.THRESH_BINARY_INV,11,2)\n",
    "    ret, res = cv2.threshold(th3, minValue, 255, cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fccb6aa-fc8d-4c39-83d7-f40e02a8241b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9fc0674d-c0d8-4b52-b622-98335dac5b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GUI:\n",
    "    \n",
    "    def __init__(self, title, size):\n",
    "        self.root = Tk()\n",
    "        self.root.title(title)\n",
    "        self.root.geometry(size)\n",
    "\n",
    "    def create_frame(self, width, height, anchor, relx, rely, background='white'):\n",
    "        frame = Frame(self.root, bg=background, width=width, height=height)\n",
    "        frame.place(anchor=anchor, relx=relx, rely=rely)\n",
    "        return frame\n",
    "        \n",
    "    def create_labels(self, label_num, labels, anchor, relx, rely, x_spacing=0, y_spacing=0, create_entrybox_per_label=False):\n",
    "        entry_labels = {}\n",
    "        entry_boxes = {}\n",
    "        relx = relx\n",
    "        rely = rely\n",
    "\n",
    "        longest_label_spacing = len(max(labels, key=len))/100.0\n",
    "        \n",
    "        for i in range(label_num):\n",
    "            label = Label(self.root, text = labels[i]+\": \",\n",
    "                           font = (\"TimesNewRoman\", 15))\n",
    "            label.place(anchor=anchor, relx=relx, rely=rely)\n",
    "            \n",
    "            entry_labels[labels[i]] = label\n",
    "            if create_entrybox_per_label:\n",
    "                entry_box = Text(self.root, font=(\"TimesNewRoman\", 20), height=1, width=10)\n",
    "                entry_box.place(anchor=anchor, relx=relx+longest_label_spacing+0.02, rely=rely)\n",
    "                \n",
    "                entry_boxes[labels[i]+'_entrybox'] = entry_box\n",
    "            rely += y_spacing\n",
    "            relx += x_spacing\n",
    "        return entry_labels, entry_boxes\n",
    "\n",
    "    def create_buttons(self, button_num, text, anchor, relx, rely, command=None, x_spacing=0, y_spacing=0):\n",
    "        buttons = {}\n",
    "        relx = relx\n",
    "        rely = rely\n",
    "        \n",
    "        for i in range(button_num):\n",
    "            btn = Button(self.root, command=command, text=text[i])\n",
    "            btn.place(anchor=anchor, relx=relx, rely=rely)\n",
    "\n",
    "            buttons[text[i]+' button'] = btn\n",
    "            \n",
    "            rely += y_spacing\n",
    "            relx += x_spacing\n",
    "\n",
    "        return buttons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5bb07d5e-fcbd-42f6-b6c4-54d8de6e0ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# mp_drawing = mp.solutions.drawing_utils\n",
    "# mp_hands = mp.solutions.hands\n",
    "# cap = None\n",
    "\n",
    "# classifier = Model.load_classifier('saved_model.h5')\n",
    "\n",
    "# def draw_region(image, center):\n",
    "#     cropped_image = cv2.rectangle(image, (center[0] - 130, center[1] - 130),\n",
    "#         (center[0] + 130, center[1] + 130), (0, 0, 255), 2)\n",
    "#     return cropped_image[center[1]-130:center[1]+130, center[0]-130:center[0]+130], cropped_image\n",
    "\n",
    "# def start_gui(title, size):\n",
    "#     gui = GUI(title, size)\n",
    "\n",
    "#     gui_frame = gui.create_frame(600, 600, 'ne', 1, 0, 'green')\n",
    "#     vid_label = Label(gui_frame)\n",
    "#     vid_label.grid()\n",
    "    \n",
    "#     return gui, vid_label\n",
    "\n",
    "# def exit_app(gui, cap):\n",
    "#     gui.root.destroy()\n",
    "#     cap.release()\n",
    "\n",
    "\n",
    "# def update_frame(image, vid_label):\n",
    "#     image_fromarray = Image.fromarray(image)\n",
    "#     imgtk = ImageTk.PhotoImage(image=image_fromarray)\n",
    "    \n",
    "#     vid_label.imgtk = imgtk\n",
    "#     vid_label.config(image=imgtk)\n",
    "\n",
    "# def get_threshold(label_entrybox):\n",
    "#     value = label_entrybox.get('1.0', END)\n",
    "#     try:\n",
    "#         return float(value)\n",
    "#     except:\n",
    "#         return 0.95\n",
    "\n",
    "\n",
    "# def get_char(gesture):\n",
    "#     classes = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L',\n",
    "#            'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V',\n",
    "#            'W', 'X', 'Y', 'Z', 'del', 'nothing', 'space']\n",
    "\n",
    "#     return Model.predict(classes, classifier, gesture)\n",
    "\n",
    "\n",
    "# def AddCharToWord(word, curr_char):\n",
    "#     temp_word = word\n",
    "#     if curr_char == 'space':\n",
    "#         #print(Auto_Correct(temp_word))\n",
    "#         temp_word = \"\"\n",
    "#     elif curr_char == 'del':\n",
    "#         temp_word = temp_word[0:-1]\n",
    "#         print('character has been deleted')\n",
    "#     elif curr_char != 'nothing':\n",
    "#         temp_word += curr_char.lower()\n",
    "#         print('character has been added: ', curr_char.lower())\n",
    "\n",
    "#     return [temp_word, curr_char]\n",
    "\n",
    "\n",
    "# def frame_video_stream(names, curr_char, prev_char, word, sentence, *args):\n",
    "#     kwargs = dict(zip(names, args))\n",
    "    \n",
    "#     threshold = get_threshold(kwargs['th_box'])\n",
    "#     curr_char = curr_char\n",
    "#     prev_char = prev_char\n",
    "    \n",
    "#     success, frame = cap.read()\n",
    "#     frame = cv2.flip(frame, 1)\n",
    "#     # Flip the image horizontally for a later selfie-view display, and convert\n",
    "#     # the BGR image to RGB.\n",
    "#     image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "#     update_frame(image, kwargs['vid_label'])\n",
    "\n",
    "#     image.flags.writeable = False\n",
    "#     results = kwargs['hands'].process(image)\n",
    "\n",
    "#     # Draw the hand annotations on the image.\n",
    "#     image.flags.writeable = True\n",
    "#     image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "#     image_height, image_width, _ = image.shape\n",
    "\n",
    "#     if results.multi_hand_landmarks:\n",
    "        \n",
    "#         for hand_landmarks in results.multi_hand_landmarks:\n",
    "#             x = [landmark.x for landmark in hand_landmarks.landmark]\n",
    "#             y = [landmark.y for landmark in hand_landmarks.landmark]\n",
    "\n",
    "            \n",
    "#             center = np.array([np.mean(x) * image_width, np.mean(y) * image_height]).astype('int32')\n",
    "#             image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "#             cropped_img, full_img = draw_region(image, center)\n",
    "\n",
    "#             update_frame(full_img, kwargs['vid_label'])\n",
    "\n",
    "#             try:\n",
    "#                 gray = cv2.cvtColor(cropped_img, cv2.COLOR_BGR2GRAY)\n",
    "#                 gray = DataGatherer().edge_detection(gray)\n",
    "\n",
    "#                 curr_char, pred = get_char(gray)\n",
    "#                 char = cv2.putText(full_img, curr_char, (center[0]-135, center[1]-135), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "#                 char_prob = cv2.putText(full_img, '{0:.2f}'.format(np.max(pred)), (center[0]+60, center[1]-135), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "#                 update_frame(full_img, kwargs['vid_label'])\n",
    "\n",
    "#                 kwargs['cc_box'].delete('1.0', 'end')\n",
    "#                 kwargs['cc_box'].insert('end', curr_char)\n",
    "                \n",
    "#                 #compare the current char with the previous one and if matched, then won't add the current char\n",
    "#                 #because the model catches the chars realy quick and if the below if statement removed,\n",
    "#                 #the current char will be added endlessly to the word\n",
    "\n",
    "#                 #also we use the threshold to prevent the meaningless characters to be added to the word\n",
    "#                 #as the program catches the motion of the user's hand when the user changes the gesture(the motion between the gestures)\n",
    "#                 #and the program thinks\n",
    "#                 #it's a gesture and tries to match it with some letter but with low probability\n",
    "#                 if (curr_char != prev_char) and (np.max(pred) > threshold):\n",
    "#                     #the below print statement is related to the formatter\n",
    "#                     #print(pred)\n",
    "#                     temp = AddCharToWord(word, curr_char)\n",
    "#                     kwargs['ow_box'].insert('end', curr_char)\n",
    "                    \n",
    "#                     if (temp[0] == \"\") and (temp[1] != \"del\"):\n",
    "#                         sentence += Auto_Correct(word) + \" \"\n",
    "#                         kwargs['sent_box'].insert('end', Auto_Correct(word) + \" \")\n",
    "#                         kwargs['ow_box'].delete('1.0', 'end')\n",
    "#                         kwargs['cw_box'].delete('1.0', 'end')\n",
    "#                         kwargs['cw_box'].insert('end', Auto_Correct(word))\n",
    "#                     word = temp[0]\n",
    "\n",
    "#                     prev_char = curr_char\n",
    "#             except:\n",
    "#                 pass\n",
    "    \n",
    "#     kwargs['vid_label'].after(1, frame_video_stream, names, curr_char, prev_char, word, sentence, *args)\n",
    "    \n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageTk\n",
    "from tkinter import Label, END\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands\n",
    "cap = None\n",
    "\n",
    "classifier = Model.load_classifier('saved_model.h5')  # Ensure the model is loaded properly\n",
    "\n",
    "\n",
    "def preprocess_image(image):\n",
    "    \"\"\"Resize and normalize the image for prediction.\"\"\"\n",
    "    image = cv2.resize(image, (128, 128))  # Assuming model input size is 128x128\n",
    "    image = image / 255.0  # Normalize pixel values\n",
    "    image = np.expand_dims(image, axis=0)  # Add batch dimension\n",
    "    image = np.expand_dims(image, axis=-1)  # Add channel dimension if grayscale\n",
    "    return image\n",
    "\n",
    "\n",
    "def draw_region(image, center):\n",
    "    \"\"\"Draw a bounding box around the hand and return the cropped region.\"\"\"\n",
    "    try:\n",
    "        x1, y1 = max(0, center[0] - 130), max(0, center[1] - 130)\n",
    "        x2, y2 = min(image.shape[1], center[0] + 130), min(image.shape[0], center[1] + 130)\n",
    "        cropped_image = image[y1:y2, x1:x2]\n",
    "        cv2.rectangle(image, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
    "        return cropped_image, image\n",
    "    except Exception as e:\n",
    "        print(f\"Error cropping image: {e}\")\n",
    "        return None, image\n",
    "\n",
    "\n",
    "predictions_queue = []\n",
    "\n",
    "def get_char(image):\n",
    "    global predictions_queue\n",
    "    classes = ['A', 'B', 'C', ..., 'space']  # Ensure all classes are listed\n",
    "    try:\n",
    "        processed_image = preprocess_image(image)\n",
    "        predictions = classifier.predict(processed_image)\n",
    "        predictions_queue.append(predictions[0])\n",
    "        if len(predictions_queue) > 5:  # Average over last 5 predictions\n",
    "            predictions_queue.pop(0)\n",
    "        avg_predictions = np.mean(predictions_queue, axis=0)\n",
    "        predicted_index = np.argmax(avg_predictions)\n",
    "        return classes[predicted_index], avg_predictions[predicted_index]\n",
    "    except Exception as e:\n",
    "        print(f\"Prediction error: {e}\")\n",
    "        return \"nothing\", 0.0\n",
    "\n",
    "        \n",
    "        \n",
    "# def get_char(image):\n",
    "#     \"\"\"Predict the character from the cropped hand image.\"\"\"\n",
    "#     classes = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L',\n",
    "#                'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V',\n",
    "#                'W', 'X', 'Y', 'Z', 'del', 'nothing', 'space']\n",
    "#     try:\n",
    "#         processed_image = preprocess_image(image)\n",
    "#         predictions = classifier.predict(processed_image)\n",
    "#         predicted_index = np.argmax(predictions)\n",
    "#         return classes[predicted_index], predictions[0][predicted_index]\n",
    "#     except Exception as e:\n",
    "#         print(f\"Prediction error: {e}\")\n",
    "#         return \"nothing\", 0.0\n",
    "\n",
    "\n",
    "def frame_video_stream(names, curr_char, prev_char, word, sentence, *args):\n",
    "    kwargs = dict(zip(names, args))\n",
    "    threshold = get_threshold(kwargs['th_box'])\n",
    "    success, frame = cap.read()\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    update_frame(image, kwargs['vid_label'])\n",
    "\n",
    "    image.flags.writeable = False\n",
    "    results = kwargs['hands'].process(image)\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    image_height, image_width, _ = image.shape\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            x = [landmark.x for landmark in hand_landmarks.landmark]\n",
    "            y = [landmark.y for landmark in hand_landmarks.landmark]\n",
    "            center = np.array([np.mean(x) * image_width, np.mean(y) * image_height]).astype('int32')\n",
    "\n",
    "            cropped_img, full_img = draw_region(image, center)\n",
    "            if cropped_img is not None:\n",
    "                # Enhanced preprocessing\n",
    "                gray = cv2.cvtColor(cropped_img, cv2.COLOR_BGR2GRAY)\n",
    "                gray = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "                curr_char, pred_prob = get_char(gray)\n",
    "\n",
    "                if (curr_char != prev_char) and (pred_prob > threshold):\n",
    "                    temp = AddCharToWord(word, curr_char)\n",
    "                    if temp[0] == \"\" and temp[1] != \"del\":\n",
    "                        sentence += Auto_Correct(word) + \" \"\n",
    "                        kwargs['sent_box'].insert('end', Auto_Correct(word) + \" \")\n",
    "                        kwargs['cw_box'].delete('1.0', 'end')\n",
    "                        kwargs['cw_box'].insert('end', Auto_Correct(word))\n",
    "                        kwargs['ow_box'].delete('1.0', 'end')\n",
    "                    word = temp[0]\n",
    "                    prev_char = curr_char\n",
    "\n",
    "                # Display predictions\n",
    "                cv2.putText(full_img, curr_char, (center[0] - 135, center[1] - 135), cv2.FONT_HERSHEY_SIMPLEX, 1,\n",
    "                            (0, 0, 255), 2, cv2.LINE_AA)\n",
    "                cv2.putText(full_img, f'{pred_prob:.2f}', (center[0] + 60, center[1] - 135), cv2.FONT_HERSHEY_SIMPLEX, 1,\n",
    "                            (0, 0, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "                update_frame(full_img, kwargs['vid_label'])\n",
    "\n",
    "    kwargs['vid_label'].after(1, frame_video_stream, names, curr_char, prev_char, word, sentence, *args)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def frame_video_stream(names, curr_char, prev_char, word, sentence, *args):\n",
    "#     kwargs = dict(zip(names, args))\n",
    "#     threshold = get_threshold(kwargs['th_box'])\n",
    "#     success, frame = cap.read()\n",
    "#     frame = cv2.flip(frame, 1)\n",
    "#     image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "#     update_frame(image, kwargs['vid_label'])\n",
    "\n",
    "#     image.flags.writeable = False\n",
    "#     results = kwargs['hands'].process(image)\n",
    "#     image.flags.writeable = True\n",
    "#     image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "#     image_height, image_width, _ = image.shape\n",
    "\n",
    "#     if results.multi_hand_landmarks:\n",
    "#         for hand_landmarks in results.multi_hand_landmarks:\n",
    "#             x = [landmark.x for landmark in hand_landmarks.landmark]\n",
    "#             y = [landmark.y for landmark in hand_landmarks.landmark]\n",
    "#             center = np.array([np.mean(x) * image_width, np.mean(y) * image_height]).astype('int32')\n",
    "\n",
    "#             cropped_img, full_img = draw_region(image, center)\n",
    "#             if cropped_img is not None:\n",
    "#                 gray = cv2.cvtColor(cropped_img, cv2.COLOR_BGR2GRAY)\n",
    "#                 curr_char, pred_prob = get_char(gray)\n",
    "\n",
    "#                 if (curr_char != prev_char) and (pred_prob > threshold):\n",
    "#                     temp = AddCharToWord(word, curr_char)\n",
    "#                     if temp[0] == \"\" and temp[1] != \"del\":\n",
    "#                         sentence += Auto_Correct(word) + \" \"\n",
    "#                         kwargs['sent_box'].insert('end', Auto_Correct(word) + \" \")\n",
    "#                         kwargs['cw_box'].delete('1.0', 'end')\n",
    "#                         kwargs['cw_box'].insert('end', Auto_Correct(word))\n",
    "#                         kwargs['ow_box'].delete('1.0', 'end')\n",
    "#                     word = temp[0]\n",
    "#                     prev_char = curr_char\n",
    "\n",
    "#                 cv2.putText(full_img, curr_char, (center[0] - 135, center[1] - 135), cv2.FONT_HERSHEY_SIMPLEX, 1,\n",
    "#                             (0, 0, 255), 2, cv2.LINE_AA)\n",
    "#                 cv2.putText(full_img, f'{pred_prob:.2f}', (center[0] + 60, center[1] - 135), cv2.FONT_HERSHEY_SIMPLEX, 1,\n",
    "#                             (0, 0, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "#                 update_frame(full_img, kwargs['vid_label'])\n",
    "\n",
    "#     kwargs['vid_label'].after(1, frame_video_stream, names, curr_char, prev_char, word, sentence, *args)\n",
    "\n",
    "\n",
    "# GUI and pipeline setup remain unchanged.\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "def pipe_cam(gui, vid_label):\n",
    "    \n",
    "    curr_char = None\n",
    "    prev_char = None\n",
    "    word = \"\"\n",
    "    sentence = \"\"\n",
    "    \n",
    "    #the predicted character won't be added to the word unless it's probability is higher than the threshold\n",
    "    #in places with good brightness and good camera the threshold can be a high value\n",
    "    #otherwise it should be a low value and the reason for that is in places that meet\n",
    "    #the above requirements, the model predict the letters with high probability to be the correct letter the user meant to add\n",
    "    threshold = float(0.65)\n",
    " \n",
    "\n",
    "    #this formatter is to print the probability of the letters in readable\n",
    "    \n",
    "    float_formatter = \"{:.5f}\".format\n",
    "    np.set_printoptions(formatter={'float_kind':float_formatter})\n",
    "    \n",
    "    global cap\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    labels_num = 5\n",
    "    labels = ['threshold', 'current char', 'original word', 'corrected word', 'sentence']\n",
    "\n",
    "    Labels, entryboxes = gui.create_labels(labels_num, labels, 'nw', 0, 0, y_spacing=0.06, create_entrybox_per_label=1)\n",
    "\n",
    "    entryboxes['original word_entrybox'].config(width=18)\n",
    "    entryboxes['corrected word_entrybox'].config(width=18)\n",
    "    entryboxes['sentence_entrybox'].config(width=18, height=8)\n",
    "    \n",
    "    \n",
    "    entryboxes['threshold_entrybox'].insert('end', threshold)\n",
    "    th_entrybox = entryboxes['threshold_entrybox']\n",
    "\n",
    "\n",
    "    cc_entrybox = entryboxes['current char_entrybox']\n",
    "\n",
    "\n",
    "    ow_entrybox = entryboxes['original word_entrybox']\n",
    "\n",
    "\n",
    "    cw_entrybox = entryboxes['corrected word_entrybox']\n",
    "\n",
    "\n",
    "    sent_entrybox = entryboxes['sentence_entrybox']\n",
    "\n",
    "    \n",
    "    Exit_program_btn = gui.create_buttons(1, ['Exit'], 'center', 0.5, 0.9, command=lambda: exit_app(gui, cap))\n",
    "\n",
    "    names = ['vid_label', 'hands', 'th_box', 'cc_box', 'ow_box', 'cw_box', 'sent_box']\n",
    "    with mp_hands.Hands(\n",
    "            min_detection_confidence=0.4,\n",
    "            min_tracking_confidence=0.5,\n",
    "            max_num_hands=1) as hands:\n",
    "        \n",
    "            frame_video_stream(names, curr_char, prev_char, word, sentence, vid_label,\n",
    "                               hands,  th_entrybox, cc_entrybox, ow_entrybox, cw_entrybox, sent_entrybox)\n",
    "            gui.root.mainloop()\n",
    "\n",
    "\n",
    "title = \"Sign Language Recognition GUI\"\n",
    "size = \"1100x1100\"\n",
    "\n",
    "gui, vid_label = start_gui(title, size)\n",
    "\n",
    "pipe_cam(gui, vid_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5682a2fd-251b-4449-a0b0-6d6f2b21d099",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
